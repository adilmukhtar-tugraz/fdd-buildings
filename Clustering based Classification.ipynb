{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e4472fe-689e-467c-bd23-81e8aa24758b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PowerTransformer\n",
    "import json \n",
    "base_dir = 'results\\\\hold_out_set_and_predicted_faults\\\\'\n",
    "import pickle\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import json\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3157ef60-3343-4985-bcb3-3c5ff2fc491b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name):\n",
    "    df = pd.read_csv(base_dir+file_name)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def train_binary_classifier_for_clusters(cluster_labels, data, model_name='logistic_regression', test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Trains a binary classifier for each cluster.\n",
    "\n",
    "    Parameters:\n",
    "    - cluster_labels: Cluster labels assigned by KMeans or any clustering algorithm\n",
    "    - data: Input data (numpy array or pandas DataFrame)\n",
    "    - model_name: Name of the binary classifier ('logistic_regression', 'random_forest', 'svm')\n",
    "    - test_size: Proportion of the dataset to include in the test split (default is 0.2)\n",
    "    - random_state: Seed for random number generation (default is 42)\n",
    "\n",
    "    Returns:\n",
    "    - classifiers: Dictionary containing trained classifiers for each cluster\n",
    "    - results: Dictionary containing evaluation scores for each cluster\n",
    "    \"\"\"\n",
    "    # Initialize dictionaries to store classifiers and accuracy scores\n",
    "    classifiers = {}\n",
    "    accuracy_scores = {}\n",
    "    # Initialize dictionary to store results\n",
    "    \n",
    "    # Identify unique clusters\n",
    "    unique_clusters = set(cluster_labels)\n",
    "    results = {}\n",
    "    for cluster in unique_clusters:\n",
    "        print(f\"Cluster Number: {cluster}\")\n",
    "        \n",
    "        #print(cluster)\n",
    "        # Select data points for the current cluster\n",
    "        cluster_indices = (cluster_labels == cluster)\n",
    "        X_cluster, y_cluster = data[cluster_indices], np.zeros(sum(cluster_indices))  # Target label for the cluster is 0\n",
    "\n",
    "        # Select an equal number of data points from other clusters\n",
    "        other_clusters = list(set(unique_clusters) - {cluster})\n",
    "\n",
    "        \n",
    "        other_indices = []\n",
    "        for other_cluster in other_clusters:\n",
    "            other_cluster_indices = np.where(cluster_labels == other_cluster)[0]\n",
    "            random_other_indices = np.random.choice(other_cluster_indices, int(sum(cluster_indices)/2), replace=True)\n",
    "            other_indices.extend(random_other_indices)\n",
    "\n",
    "        X_other, y_other = data[other_indices], np.ones(sum(cluster_indices))  # Target label for other clusters is 1\n",
    "\n",
    "        # Combine data for the current cluster and other clusters\n",
    "        X_combined = np.vstack([X_cluster, X_other])\n",
    "        y_combined = np.hstack([y_cluster, y_other])\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_combined, y_combined, test_size=test_size, random_state=random_state)\n",
    "\n",
    "        # Train the specified binary classifier\n",
    "        if model_name == 'logistic_regression':\n",
    "            classifier = LogisticRegression(random_state=random_state)\n",
    "        elif model_name == 'random_forest':\n",
    "            classifier = RandomForestClassifier(random_state=random_state)\n",
    "        elif model_name == 'svm':\n",
    "            classifier = SVC(random_state=random_state)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid model_name. Choose from 'logistic_regression', 'random_forest', or 'svm'.\")\n",
    "\n",
    "        classifier.fit(X_train, y_train)\n",
    "\n",
    "        # Evaluate the classifier on the test set\n",
    "        y_pred = classifier.predict(X_test)\n",
    "\n",
    "        classifiers[str(cluster)] = classifier\n",
    "        results[str(cluster)] = {'accuracy': accuracy_score(y_test, y_pred), 'precision': precision_score(y_test, y_pred), \n",
    "                                 'recall': recall_score(y_test, y_pred), 'f1': f1_score(y_test, y_pred)}\n",
    "\n",
    "    return classifiers, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75015007-6d81-45ef-9f4f-e99ef4c7b6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'LOF_predicted_faults_vals_sampling_freq_5T.csv'\n",
    "df_anomaly_data = load_data(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1d1373a-74cf-4493-a9dd-ed23e15602e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anomaly_deduplicated = df_anomaly_data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3841f7fe-e481-4d73-9fd3-83ca9b709789",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_labels = np.load('models/cluster_labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "97001829-de23-4a0b-9f6d-a374532f810c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Number: 0\n",
      "Cluster Number: 1\n",
      "Cluster Number: 2\n"
     ]
    }
   ],
   "source": [
    "classifiers, results = train_binary_classifier_for_clusters(loaded_labels, df_anomaly_deduplicated.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8e814919-d359-4d7b-9f04-a5ece8e768af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': LogisticRegression(random_state=42),\n",
       " '1': LogisticRegression(random_state=42),\n",
       " '2': LogisticRegression(random_state=42)}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f7e3d1e9-7ad8-4718-9496-9b8765196705",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/logit_regression_cluster_models_results.json', \"w\") as outfile: \n",
    "        json.dump(results, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "49457067-14a7-49c1-b279-2a22254777f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(random_state=42)\n",
      "LogisticRegression(random_state=42)\n",
      "LogisticRegression(random_state=42)\n"
     ]
    }
   ],
   "source": [
    "for cluster in classifiers.keys():\n",
    "    print(classifiers[cluster])\n",
    "    joblib.dump(classifiers[cluster], 'models/logit_reg_cluster_'+cluster+'.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28a15ca-804f-425d-8d84-62498a2d9c06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
