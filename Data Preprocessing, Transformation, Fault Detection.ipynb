{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6651bca-0bde-4b1d-aebf-17852cc9b494",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PowerTransformer\n",
    "import json \n",
    "base_dir = 'dataset\\\\LBNL_FDD_Dataset_SDAHU\\\\'\n",
    "import pickle\n",
    "from sklearn.metrics import precision_recall_curve, auc,confusion_matrix\n",
    "import json\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import random\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.neighbors import LocalOutlierFactor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd6f4358-5970-493e-982a-cfd9d52edcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name):\n",
    "    df = pd.read_csv(base_dir+file_name, index_col='Datetime')\n",
    "    df.index = pd.to_datetime(df.index, format='%Y-%m-%d %H:%M:%S')\n",
    "    return df\n",
    "\n",
    "def plot_line_graph(df1, df2, feature):\n",
    "    plt.plot(df1[feature].values, 'g')\n",
    "    plt.plot(df2[feature].values, 'r')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def check_stationarity(data, column_name, significance_level=0.05):\n",
    "    \"\"\"\n",
    "    Check the stationarity of a time series using the Augmented Dickey-Fuller (ADF) test.\n",
    "\n",
    "    Parameters:\n",
    "    - data: pd.DataFrame\n",
    "        Input DataFrame with time series data.\n",
    "    - column_name: str\n",
    "        Name of the column containing the time series data.\n",
    "    - significance_level: float, optional (default=0.05)\n",
    "        Significance level for the ADF test.\n",
    "\n",
    "    Returns:\n",
    "    - bool\n",
    "        True if the time series is stationary, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if the specified column exists in the DataFrame\n",
    "    if column_name not in data.columns:\n",
    "        raise ValueError(f\"Column '{column_name}' not found in the DataFrame.\")\n",
    "\n",
    "    try:\n",
    "        # Perform the ADF test\n",
    "        result = adfuller(data[column_name])\n",
    "\n",
    "        # Extract ADF test statistic and p-value\n",
    "        adf_statistic = result[0]\n",
    "        p_value = result[1]\n",
    "    \n",
    "        # Compare p-value with significance level\n",
    "        is_stationary = p_value <= significance_level\n",
    "        \n",
    "        if stationary:\n",
    "            print(f\"The time series is stationary. ADF Statistic: {adf_statistic}, p-value: {p_value}\")\n",
    "        else:\n",
    "            print(f\"The time series is not stationary. ADF Statistic: {adf_statistic}, p-value: {p_value}\")\n",
    "            \n",
    "        #return is_stationary, adf_statistic, p_value\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "def drop_low_variance_features(data, threshold=0.01):\n",
    "    \n",
    "    # Calculate variance of each feature\n",
    "    variances = data.var()\n",
    "\n",
    "    #print(\"Variances of features:\")\n",
    "    #print(variances)\n",
    "\n",
    "    # Use VarianceThreshold to identify low variance features\n",
    "    selector = VarianceThreshold(threshold)\n",
    "    selector.fit(data)\n",
    "\n",
    "    # Get indices of features to keep\n",
    "    keep_indices = selector.get_support(indices=True)\n",
    "\n",
    "    # Subset the DataFrame with selected features\n",
    "    selected_data = data.iloc[:, keep_indices]\n",
    "\n",
    "    return selected_data\n",
    "\n",
    "\n",
    "def evaluate_classification_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Evaluate classification metrics such as accuracy, precision, recall, and F1 score.\n",
    "\n",
    "    Parameters:\n",
    "    - y_test: list or array-like\n",
    "        True labels.\n",
    "    - y_pred: list or array-like\n",
    "        Predicted labels.\n",
    "\n",
    "    Returns:\n",
    "    - dict\n",
    "        Dictionary containing evaluation metrics.\n",
    "    \"\"\"\n",
    "    # Initialize counts\n",
    "    TP = TN = FP = FN = 0\n",
    "\n",
    "    # Calculate confusion matrix elements\n",
    "    for true, predicted in zip(y_true, y_pred):\n",
    "        if true == 1 and predicted == 1:\n",
    "            TP += 1\n",
    "        elif true == 0 and predicted == 0:\n",
    "            TN += 1\n",
    "        elif true == 0 and predicted == 1:\n",
    "            FP += 1\n",
    "        elif true == 1 and predicted == 0:\n",
    "            FN += 1\n",
    "\n",
    "    print(\"TP:\", TP)\n",
    "    print(\"TN:\", TN)\n",
    "    print(\"FP:\", FP)\n",
    "    print(\"FN:\", FN)\n",
    "\n",
    "    # Compute classification metrics\n",
    "    accuracy = round(accuracy_score(y_true, y_pred), 2)\n",
    "    precision = round(precision_score(y_true, y_pred))\n",
    "    recall = round(recall_score(y_true, y_pred), 2)\n",
    "    f1 = round(f1_score(y_true, y_pred), 2)\n",
    "\n",
    "    # Create a dictionary to store the metrics\n",
    "    metrics_dict = {\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1\n",
    "    }\n",
    "\n",
    "    return metrics_dict\n",
    "\n",
    "def fine_tune_model(param_grid, model, X):\n",
    "    # Create GridSearchCV object\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', verbose=True)\n",
    "\n",
    "    # Perform grid search on non-faulty data\n",
    "    grid_search.fit(X)\n",
    "\n",
    "    # Get the best Isolation Forest model\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "def cv_for_test_set_eval(dict_fault, X, freq, splits, model_name):\n",
    "    tscv = TimeSeriesSplit(n_splits=splits)  # You can adjust the number of splits\n",
    "    results = {}\n",
    "    counter = 1\n",
    "    for file in dict_fault.keys():\n",
    "        if 'short' in file:\n",
    "            continue\n",
    "        else:\n",
    "            results[file] = {}\n",
    "            results[file]['Accuracy'] = []\n",
    "            results[file]['F1'] = []\n",
    "            results[file]['Precision'] = []\n",
    "            results[file]['Recall'] = []\n",
    "            \n",
    "    for train_index, test_index in tscv.split(X):\n",
    "        \n",
    "        X_train = X[train_index]\n",
    "        train_fft = np.fft.fft(X_train)\n",
    "        scaler = MinMaxScaler()\n",
    "        train_scaled_fft = scaler.fit_transform(train_fft.real)\n",
    "\n",
    "        # Train the specified binary classifier\n",
    "        if model_name == 'lof':\n",
    "            param_grid = {'n_neighbors': [5, 10, 15, 20, 25, 30, 35, 40], 'contamination': [0.01, 0.05, 0.1, 0.2]}\n",
    "            lof = LocalOutlierFactor(novelty=True)\n",
    "            trained_model = fine_tune_model(param_grid, lof, train_scaled_fft)\n",
    "            \n",
    "        elif model_name == 'isolation_forest':\n",
    "            param_grid = {\n",
    "                        'n_estimators': [50, 100, 150, 200, 250, 300],\n",
    "                        'max_samples': ['auto', 0.5, 0.7],\n",
    "                        'contamination': [0.05, 0.1, 0.2],\n",
    "                        'max_features': [1.0, 0.8, 0.6, 0.4, 0.3, 0.2],\n",
    "                        'bootstrap': [True, False]\n",
    "                        }\n",
    "            # Initialize Isolation Forest\n",
    "            isolation_forest = IsolationForest(random_state=42)\n",
    "            trained_model = fine_tune_model(param_grid, isolation_forest, train_scaled_fft)\n",
    "\n",
    "        \n",
    "        elif model_name == 'elliptic':\n",
    "            param_grid = {\n",
    "                        'contamination': [0.05, 0.1, 0.2],\n",
    "                        'store_precision': [True, False],\n",
    "                        'assume_centered': [True, False],\n",
    "                        'support_fraction': [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "                        }\n",
    "\n",
    "            # Initialize Elliptic Envelope\n",
    "            elliptic_envelope = EllipticEnvelope()\n",
    "            trained_model = fine_tune_model(param_grid, elliptic_envelope, train_scaled_fft)\n",
    "        \n",
    "        elif model_name == 'onesvm':\n",
    "            # Initialize One-Class SVM        \n",
    "            param_grid = {\n",
    "                'nu': [0.01, 0.05, 0.1, 0.2],\n",
    "                'kernel': ['rbf', 'linear', 'poly', 'sigmoid'],\n",
    "                'gamma': ['scale', 'auto']\n",
    "                \n",
    "            }\n",
    "            one_class_svm = OneClassSVM()\n",
    "            trained_model = fine_tune_model(param_grid, one_class_svm, train_scaled_fft)\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"Invalid model_name. Choose from 'log', or 'isolation_forest'.\")\n",
    "        \n",
    "        print(trained_model)\n",
    "        for file in dict_fault.keys():\n",
    "            if 'short' in file:\n",
    "                continue\n",
    "            else:\n",
    "                df_test_resampled = dict_fault[file].resample(freq).mean()\n",
    "               \n",
    "                \n",
    "                X_test = df_test_resampled.values[test_index]\n",
    "                X_non_faulty = X[test_index]\n",
    "                \n",
    "                X_test_fault_non_fault = np.vstack((X_test, X_non_faulty))\n",
    "                \n",
    "                y_test = [1]*X_test.shape[0]\n",
    "                y_test.extend([0]*X_non_faulty.shape[0])\n",
    "                \n",
    "                test_fft = np.fft.fft(X_test_fault_non_fault)\n",
    "                test_pred_labels = trained_model.predict(scaler.transform(test_fft.real))\n",
    "                predicted_labels = np.where(test_pred_labels == -1, 1, 0)\n",
    "                \n",
    "                metrics = evaluate_classification_metrics(y_test, predicted_labels)\n",
    "                print(counter, metrics)\n",
    "                results[file]['Accuracy'].append(metrics['Accuracy'])\n",
    "                results[file]['F1'].append(metrics['F1 Score'])\n",
    "                results[file]['Precision'].append(metrics['Precision'])\n",
    "                results[file]['Recall'].append(metrics['Recall'])\n",
    "        print('*'*100)\n",
    "        counter+=1\n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "450b78e2-2ca5-47e6-aab9-2707f4608ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_data = load_data('AHU_annual.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d4699f1-8dc4-4766-b025-71cab2a077a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_correct_dropped_variance_features = drop_low_variance_features(correct_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fc933d8-6d55-4141-8754-c1f6a5a41f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#selected_features = list(df_correct_dropped_variance_features.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4807aa7d-7be6-4c5a-802c-2cfd06e8ed61",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_faulty_df = {}\n",
    "for c, file in enumerate(os.listdir(base_dir)):\n",
    "    if c>0:\n",
    "        dict_faulty_df[file] = load_data(file)\n",
    "        #dict_faulty_df[file] = df[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1aa0cb7-bae7-48a9-8153-93b89b1d6f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_start_date_time = '2018-01-01 01:00:00'\n",
    "training_end_date_time = '2018-10-31 23:59:00'\n",
    "\n",
    "test_start_data = '2018-11-01 00:00:00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37b5b0b3-c402-460b-a5bb-990e6aaf8f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "resampling_freq = '5T'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c874f4a5-7864-4156-9ecd-835065501f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_resampled = correct_data.resample(resampling_freq).mean()\n",
    "X = df_train_resampled.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb78249d-144e-426e-aa26-e075dc4047bd",
   "metadata": {},
   "source": [
    "### 10 Fold Time Series Evaluation of OCC Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fe0728-fe65-46a3-aea3-5098ec7bb0b6",
   "metadata": {},
   "source": [
    "#### Local Outlier Factor (LOF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11188c3-d823-44ba-ab90-b232d010a54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_cv_results_lof = cv_for_test_set_eval(dict_faulty_df, X, resampling_freq, 10, 'lof')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99693735-6517-44bc-845f-c95b262940e5",
   "metadata": {},
   "source": [
    "#### Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967fa76d-3bc0-4be4-b00c-d3d99d909ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_cv_results_isolation_forest = cv_for_test_set_eval(dict_faulty_df, X, resampling_freq, 10, 'isolation_forest')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3216278-e2b5-4296-b04f-42820c54a492",
   "metadata": {},
   "source": [
    "#### Elliptic Envelope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099a85dd-319c-43cf-823c-ea3c9164ce46",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_cv_results_eliptic = cv_for_test_set_eval(dict_faulty_df, X, resampling_freq, 10, 'elliptic')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d4abfb-01a6-4269-99c2-c51f5a835a1e",
   "metadata": {},
   "source": [
    "#### One Class SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b9c8f4-ba6f-468b-98f7-1483a15605a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_cv_results_onesvm = cv_for_test_set_eval(dict_faulty_df, X, resampling_freq, 10, 'onesvm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70516235-685e-4b3a-bf23-acad9f160886",
   "metadata": {},
   "source": [
    "### Save 10 Fold Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c58102-56cf-47dc-bc63-3f55a134bfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/10_fold_faults_non_faults_evaluation_lof.json', \"w\") as outfile: \n",
    "        json.dump(test_set_cv_results_lof, outfile, indent=4)\n",
    "with open('results/10_fold_faults_non_faults_evaluation_isolation.json', \"w\") as outfile: \n",
    "        json.dump(test_set_cv_results_isolation_forest, outfile, indent=4)\n",
    "with open('results/10_fold_faults_non_faults_evaluation_eliptic.json', \"w\") as outfile: \n",
    "        json.dump(test_set_cv_results_eliptic, outfile, indent=4)\n",
    "with open('results/10_fold_faults_non_faults_evaluation_onesvm.json', \"w\") as outfile: \n",
    "        json.dump(test_set_cv_results_onesvm, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe926e0a-ee8c-4253-a05b-08920402d533",
   "metadata": {},
   "source": [
    "### Trained Best Model\n",
    "In our case, Eliptic Envelope performs, on average, better than the other OCC models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "65f58138-57a6-4f84-aed4-47a69a292784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>EllipticEnvelope(contamination=0.05)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">EllipticEnvelope</label><div class=\"sk-toggleable__content\"><pre>EllipticEnvelope(contamination=0.05)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "EllipticEnvelope(contamination=0.05)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = correct_data[training_start_date_time : training_end_date_time].resample(resampling_freq).mean()\n",
    "X_train = df_train.values\n",
    "\n",
    "model_eliptic_envelop = EllipticEnvelope(contamination=0.05)\n",
    "X_train_fft = np.fft.fft(X_train)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train_fft_scaled = scaler.fit_transform(X_train_fft.real)\n",
    "\n",
    "model_eliptic_envelop.fit(X_train_fft_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d20c61a5-7f54-4d0d-8b9a-cb48d9981aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate_model(df_faulty_dict, model, scaler):\n",
    "    \n",
    "    for file in df_faulty_dict.keys():\n",
    "        print(file)\n",
    "        df_test = df_faulty_dict[file][test_start_data : ].resample(resampling_freq).mean()\n",
    "\n",
    "        X_test = df_test.values\n",
    "        X_test_fft = np.fft.fft(X_test)\n",
    "    \n",
    "        X_test_fft_scaled = scaler.transform(X_test_fft.real)\n",
    "        \n",
    "        y_test = np.asarray([1]*X_test.shape[0])\n",
    "    \n",
    "        test_pred_labels = model.predict(scaler.transform(X_test_fft_scaled.real))\n",
    "        predicted_labels = np.where(test_pred_labels == -1, 1, 0)\n",
    "        \n",
    "        metrics = evaluate_classification_metrics(y_test, predicted_labels)\n",
    "        print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8463c1eb-2616-4d15-8060-8779e7bd1841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coi_bias_-2_annual.csv\n",
      "TP: 17568\n",
      "TN: 0\n",
      "FP: 0\n",
      "FN: 0\n",
      "{'Accuracy': 1.0, 'Precision': 1, 'Recall': 1.0, 'F1 Score': 1.0}\n",
      "coi_bias_-4_annual.csv\n",
      "TP: 17568\n",
      "TN: 0\n",
      "FP: 0\n",
      "FN: 0\n",
      "{'Accuracy': 1.0, 'Precision': 1, 'Recall': 1.0, 'F1 Score': 1.0}\n",
      "coi_bias_2_annual.csv\n",
      "TP: 17568\n",
      "TN: 0\n",
      "FP: 0\n",
      "FN: 0\n",
      "{'Accuracy': 1.0, 'Precision': 1, 'Recall': 1.0, 'F1 Score': 1.0}\n",
      "coi_bias_4_annual.csv\n",
      "TP: 17568\n",
      "TN: 0\n",
      "FP: 0\n",
      "FN: 0\n",
      "{'Accuracy': 1.0, 'Precision': 1, 'Recall': 1.0, 'F1 Score': 1.0}\n",
      "coi_leakage_010_annual.csv\n",
      "TP: 17568\n",
      "TN: 0\n",
      "FP: 0\n",
      "FN: 0\n",
      "{'Accuracy': 1.0, 'Precision': 1, 'Recall': 1.0, 'F1 Score': 1.0}\n",
      "coi_leakage_025_annual.csv\n",
      "TP: 17568\n",
      "TN: 0\n",
      "FP: 0\n",
      "FN: 0\n",
      "{'Accuracy': 1.0, 'Precision': 1, 'Recall': 1.0, 'F1 Score': 1.0}\n",
      "coi_leakage_040_annual.csv\n",
      "TP: 17568\n",
      "TN: 0\n",
      "FP: 0\n",
      "FN: 0\n",
      "{'Accuracy': 1.0, 'Precision': 1, 'Recall': 1.0, 'F1 Score': 1.0}\n",
      "coi_leakage_050_annual.csv\n",
      "TP: 17568\n",
      "TN: 0\n",
      "FP: 0\n",
      "FN: 0\n",
      "{'Accuracy': 1.0, 'Precision': 1, 'Recall': 1.0, 'F1 Score': 1.0}\n",
      "coi_stuck_010_annual.csv\n",
      "TP: 17568\n",
      "TN: 0\n",
      "FP: 0\n",
      "FN: 0\n",
      "{'Accuracy': 1.0, 'Precision': 1, 'Recall': 1.0, 'F1 Score': 1.0}\n",
      "coi_stuck_025_annual.csv\n",
      "TP: 17568\n",
      "TN: 0\n",
      "FP: 0\n",
      "FN: 0\n",
      "{'Accuracy': 1.0, 'Precision': 1, 'Recall': 1.0, 'F1 Score': 1.0}\n",
      "coi_stuck_050_annual.csv\n",
      "TP: 17568\n",
      "TN: 0\n",
      "FP: 0\n",
      "FN: 0\n",
      "{'Accuracy': 1.0, 'Precision': 1, 'Recall': 1.0, 'F1 Score': 1.0}\n",
      "coi_stuck_075_annual.csv\n",
      "TP: 17568\n",
      "TN: 0\n",
      "FP: 0\n",
      "FN: 0\n",
      "{'Accuracy': 1.0, 'Precision': 1, 'Recall': 1.0, 'F1 Score': 1.0}\n",
      "damper_stuck_010_annual.csv\n",
      "TP: 17568\n",
      "TN: 0\n",
      "FP: 0\n",
      "FN: 0\n",
      "{'Accuracy': 1.0, 'Precision': 1, 'Recall': 1.0, 'F1 Score': 1.0}\n",
      "damper_stuck_025_annual.csv\n",
      "TP: 17568\n",
      "TN: 0\n",
      "FP: 0\n",
      "FN: 0\n",
      "{'Accuracy': 1.0, 'Precision': 1, 'Recall': 1.0, 'F1 Score': 1.0}\n",
      "damper_stuck_075_annual.csv\n",
      "TP: 17568\n",
      "TN: 0\n",
      "FP: 0\n",
      "FN: 0\n",
      "{'Accuracy': 1.0, 'Precision': 1, 'Recall': 1.0, 'F1 Score': 1.0}\n",
      "damper_stuck_100_annual_short.csv\n",
      "TP: 1\n",
      "TN: 0\n",
      "FP: 0\n",
      "FN: 0\n",
      "{'Accuracy': 1.0, 'Precision': 1, 'Recall': 1.0, 'F1 Score': 1.0}\n",
      "oa_bias_-2_annual.csv\n",
      "TP: 17568\n",
      "TN: 0\n",
      "FP: 0\n",
      "FN: 0\n",
      "{'Accuracy': 1.0, 'Precision': 1, 'Recall': 1.0, 'F1 Score': 1.0}\n",
      "oa_bias_-4_annual.csv\n",
      "TP: 17568\n",
      "TN: 0\n",
      "FP: 0\n",
      "FN: 0\n",
      "{'Accuracy': 1.0, 'Precision': 1, 'Recall': 1.0, 'F1 Score': 1.0}\n",
      "oa_bias_2_annual.csv\n",
      "TP: 17568\n",
      "TN: 0\n",
      "FP: 0\n",
      "FN: 0\n",
      "{'Accuracy': 1.0, 'Precision': 1, 'Recall': 1.0, 'F1 Score': 1.0}\n",
      "oa_bias_4_annual.csv\n",
      "TP: 17568\n",
      "TN: 0\n",
      "FP: 0\n",
      "FN: 0\n",
      "{'Accuracy': 1.0, 'Precision': 1, 'Recall': 1.0, 'F1 Score': 1.0}\n"
     ]
    }
   ],
   "source": [
    "train_evaluate_model(dict_faulty_df, model_eliptic_envelop, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "542cc06e-3c47-484b-b27e-d2687852dcf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/minmax_elliptic_envelope_mode_5T.pkl']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the model to a file\n",
    "joblib.dump(model_eliptic_envelop, 'models/elliptic_envelope_model_'+str(resampling_freq)+'.pkl')\n",
    "joblib.dump(scaler, 'models/minmax_elliptic_envelope_mode_'+str(resampling_freq)+'.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87af85f8-d3ba-44b2-83f0-fe770c93c02c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abcfb5f-b74e-429d-a49a-a6f804c843c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
