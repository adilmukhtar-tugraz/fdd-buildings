{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f6651bca-0bde-4b1d-aebf-17852cc9b494",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PowerTransformer\n",
    "import json \n",
    "base_dir = 'dataset\\\\LBNL_FDD_Dataset_SDAHU\\\\'\n",
    "import pickle\n",
    "from sklearn.metrics import precision_recall_curve, auc,confusion_matrix\n",
    "import json\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "dd6f4358-5970-493e-982a-cfd9d52edcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name):\n",
    "    df = pd.read_csv(base_dir+file_name, index_col='Datetime')\n",
    "    df.index = pd.to_datetime(df.index, format='%Y-%m-%d %H:%M:%S')\n",
    "    return df\n",
    "\n",
    "def plot_line_graph(df1, df2, feature):\n",
    "    plt.plot(df1[feature].values, 'g')\n",
    "    plt.plot(df2[feature].values, 'r')\n",
    "    plt.show()\n",
    "\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "def check_stationarity(data, column_name, significance_level=0.05):\n",
    "    \"\"\"\n",
    "    Check the stationarity of a time series using the Augmented Dickey-Fuller (ADF) test.\n",
    "\n",
    "    Parameters:\n",
    "    - data: pd.DataFrame\n",
    "        Input DataFrame with time series data.\n",
    "    - column_name: str\n",
    "        Name of the column containing the time series data.\n",
    "    - significance_level: float, optional (default=0.05)\n",
    "        Significance level for the ADF test.\n",
    "\n",
    "    Returns:\n",
    "    - bool\n",
    "        True if the time series is stationary, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if the specified column exists in the DataFrame\n",
    "    if column_name not in data.columns:\n",
    "        raise ValueError(f\"Column '{column_name}' not found in the DataFrame.\")\n",
    "\n",
    "    try:\n",
    "        # Perform the ADF test\n",
    "        result = adfuller(data[column_name])\n",
    "\n",
    "        # Extract ADF test statistic and p-value\n",
    "        adf_statistic = result[0]\n",
    "        p_value = result[1]\n",
    "    \n",
    "        # Compare p-value with significance level\n",
    "        is_stationary = p_value <= significance_level\n",
    "        \n",
    "        if stationary:\n",
    "            print(f\"The time series is stationary. ADF Statistic: {adf_statistic}, p-value: {p_value}\")\n",
    "        else:\n",
    "            print(f\"The time series is not stationary. ADF Statistic: {adf_statistic}, p-value: {p_value}\")\n",
    "            \n",
    "        #return is_stationary, adf_statistic, p_value\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "def drop_low_variance_features(data, threshold=0.01):\n",
    "    \n",
    "    # Calculate variance of each feature\n",
    "    variances = data.var()\n",
    "\n",
    "    #print(\"Variances of features:\")\n",
    "    #print(variances)\n",
    "\n",
    "    # Use VarianceThreshold to identify low variance features\n",
    "    selector = VarianceThreshold(threshold)\n",
    "    selector.fit(data)\n",
    "\n",
    "    # Get indices of features to keep\n",
    "    keep_indices = selector.get_support(indices=True)\n",
    "\n",
    "    # Subset the DataFrame with selected features\n",
    "    selected_data = data.iloc[:, keep_indices]\n",
    "\n",
    "    return selected_data\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluate_classification_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Evaluate classification metrics such as accuracy, precision, recall, and F1 score.\n",
    "\n",
    "    Parameters:\n",
    "    - y_test: list or array-like\n",
    "        True labels.\n",
    "    - y_pred: list or array-like\n",
    "        Predicted labels.\n",
    "\n",
    "    Returns:\n",
    "    - dict\n",
    "        Dictionary containing evaluation metrics.\n",
    "    \"\"\"\n",
    "    # Initialize counts\n",
    "    TP = TN = FP = FN = 0\n",
    "\n",
    "    # Calculate confusion matrix elements\n",
    "    for true, predicted in zip(y_true, y_pred):\n",
    "        if true == 1 and predicted == 1:\n",
    "            TP += 1\n",
    "        elif true == 0 and predicted == 0:\n",
    "            TN += 1\n",
    "        elif true == 0 and predicted == 1:\n",
    "            FP += 1\n",
    "        elif true == 1 and predicted == 0:\n",
    "            FN += 1\n",
    "\n",
    "    print(\"TP:\", TP)\n",
    "    print(\"TN:\", TN)\n",
    "    print(\"FP:\", FP)\n",
    "    print(\"FN:\", FN)\n",
    "\n",
    "    # Compute classification metrics\n",
    "    accuracy = round(accuracy_score(y_true, y_pred), 2)\n",
    "    precision = round(precision_score(y_true, y_pred))\n",
    "    recall = round(recall_score(y_true, y_pred), 2)\n",
    "    f1 = round(f1_score(y_true, y_pred), 2)\n",
    "\n",
    "    # Create a dictionary to store the metrics\n",
    "    metrics_dict = {\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1\n",
    "    }\n",
    "\n",
    "    return metrics_dict\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "def visualize_optimal_clusters(data, max_clusters=10):\n",
    "    \"\"\"\n",
    "    Visualize the optimal number of clusters using the elbow method.\n",
    "\n",
    "    Parameters:\n",
    "    - data: array-like\n",
    "        The training data for clustering.\n",
    "    - max_clusters: int, optional (default=10)\n",
    "        Maximum number of clusters to consider.\n",
    "\n",
    "    Returns:\n",
    "    None (plots the elbow curve).\n",
    "    \"\"\"\n",
    "\n",
    "    distortions = []\n",
    "    \n",
    "    for i in range(1, max_clusters + 1):\n",
    "        kmeans = KMeans(n_clusters=i, random_state=42)\n",
    "        kmeans.fit(data)\n",
    "        distortions.append(kmeans.inertia_)\n",
    "\n",
    "    # Plotting the elbow curve\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(range(1, max_clusters + 1), distortions, marker='o')\n",
    "    plt.title('Elbow Method for Optimal Number of Clusters')\n",
    "    plt.xlabel('Number of Clusters')\n",
    "    plt.ylabel('Distortion (Inertia)')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def predict_and_assign_labels(kmeans_model, test_data):\n",
    "    \"\"\"\n",
    "    Predict cluster labels for test data and assign binary labels.\n",
    "\n",
    "    Parameters:\n",
    "    - kmeans_model: KMeans\n",
    "        Trained KMeans model.\n",
    "    - test_data: array-like\n",
    "        Test data for prediction.\n",
    "\n",
    "    Returns:\n",
    "    - binary_labels: array-like\n",
    "        Binary labels (0 or 1) assigned based on cluster membership.\n",
    "    \"\"\"\n",
    "\n",
    "    # Predict cluster labels for the test data\n",
    "    cluster_labels = kmeans_model.predict(test_data)\n",
    "    print(\"cluster labels\",cluster_labels) \n",
    "    # Assign binary labels (0 if in any cluster, 1 if not in any cluster)\n",
    "    binary_labels = np.where(cluster_labels >= 0, 0, 1)\n",
    "\n",
    "    return binary_labels\n",
    "\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def fine_tune_lof_model(non_faulty_data):\n",
    "    \"\"\"\n",
    "    Fine-tune Local Outlier Factor (LOF) model on non-faulty data.\n",
    "\n",
    "    Parameters:\n",
    "    - non_faulty_data: pd.DataFrame or 2D array\n",
    "        Non-faulty data for training the LOF model.\n",
    "\n",
    "    Returns:\n",
    "    - Trained LOF model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Fine-tune LOF model using GridSearchCV\n",
    "    param_grid = {'n_neighbors': [5, 10, 15, 20], 'contamination': [0.01, 0.05, 0.1, 0.2]}\n",
    "    lof = LocalOutlierFactor(novelty=True)\n",
    "    grid_search = GridSearchCV(lof, param_grid, scoring='neg_mean_squared_error', cv=5, verbose=True)\n",
    "    grid_search.fit(non_faulty_data)\n",
    "\n",
    "    # Train the best LOF model with optimal hyperparameters\n",
    "    best_lof_model = LocalOutlierFactor(n_neighbors=grid_search.best_params_['n_neighbors'],\n",
    "                                        contamination=grid_search.best_params_['contamination'], novelty=True)\n",
    "    best_lof_model.fit(non_faulty_data)\n",
    "\n",
    "    return best_lof_model\n",
    "\n",
    "def evaluate_test_data(dict_fault, scaler, model, model_name, freq):\n",
    "    \n",
    "    results = {}\n",
    "    df_predict_faults_vals = pd.DataFrame()\n",
    "    df_hold_out_set_exp = pd.DataFrame()\n",
    "\n",
    "    for file in dict_fault.keys():\n",
    "        print(file)\n",
    "        \n",
    "        \n",
    "        df_test = dict_faulty_df[file].loc[training_start_date_time : training_end_date_time].resample(freq).mean()\n",
    "\n",
    "\n",
    "        df_hold_out_file = dict_faulty_df[file].loc[test_start_data:].resample(freq).mean()\n",
    "        df_hold_out_set_exp = pd.concat([df_hold_out_set_exp, df_hold_out_file])\n",
    "\n",
    "        \n",
    "        test_fft = np.fft.fft(df_test)\n",
    "        \n",
    "        y_true = [1]*len(df_test)\n",
    "        \n",
    "        labels = trained_lof_model.predict(scaler.transform(test_fft.real))\n",
    "        \n",
    "        binary_labels = np.where(labels == -1, 1, 0)\n",
    "\n",
    "        value_counts = Counter(binary_labels)\n",
    "\n",
    "        #Print the counts\n",
    "        for value, count in value_counts.items():\n",
    "            print(f\"{value}: {count} occurrences\")\n",
    "            \n",
    "        metrics = evaluate_classification_metrics(y_true, binary_labels)\n",
    "        print(metrics)\n",
    "        results[file] = metrics\n",
    "        print()\n",
    "\n",
    "        fault_assignment = np.where(binary_labels == 1, True, False)\n",
    "        \n",
    "        fault_indices = np.where(fault_assignment)[0]\n",
    "        \n",
    "        df_faults_vals = df_test.iloc[fault_indices]\n",
    "        \n",
    "        df_predict_faults_vals = pd.concat([df_predict_faults_vals, df_faults_vals], ignore_index=True)\n",
    "        \n",
    "        \n",
    "    with open('results/'+model_name+'_results'+str(freq)+'.json', \"w\") as outfile: \n",
    "        json.dump(results, outfile, indent=4)\n",
    "        \n",
    "    df_predict_faults_vals.to_csv('results/'+model_name+'_predicted_faults_vals'+'_sampling_freq_'+str(freq)+'.csv', sep=',', index=False, encoding='utf-8')\n",
    "\n",
    "    df_hold_out_set_exp.to_csv('results/hold_out_test_set_explanations_sampling_freq_'+str(freq)+'.csv', sep=',', index=True, encoding='utf-8')\n",
    "\n",
    "def load_fault_data(file_name):\n",
    "    df = pd.read_csv(base_dir+file_name, index_col='Datetime')\n",
    "    df.index = pd.to_datetime(df.index, format='%Y-%m-%d %H:%M:%S')\n",
    "    return df\n",
    "\n",
    "\n",
    "def cv_for_validation_eval(X, splits):\n",
    "    tscv = TimeSeriesSplit(n_splits=splits)  # You can adjust the number of splits\n",
    "    accuracy = []\n",
    "    for train_index, valid_index in tscv.split(X):\n",
    "    \n",
    "        X_train, X_valid = X[train_index], X[valid_index]\n",
    "        y_train, y_valid = [0]*len(X_train), [0]*len(X_valid)\n",
    "    \n",
    "        train_fft = np.fft.fft(X_train)\n",
    "        scaler = MinMaxScaler()\n",
    "        train_scaled_fft = scaler.fit_transform(train_fft.real)\n",
    "        \n",
    "        trained_lof_model = fine_tune_lof_model(train_scaled_fft)\n",
    "    \n",
    "        valid_fft = np.fft.fft(X_valid)\n",
    "        \n",
    "        valid_pred_labels = trained_lof_model.predict(scaler.transform(valid_fft.real))\n",
    "\n",
    "        predicted_labels = np.where(valid_pred_labels == -1, 1, 0)\n",
    "        \n",
    "        counter_result = Counter(list(predicted_labels))\n",
    "        print(\"Occurrences of items in NumPy array:\")\n",
    "        for item, count in counter_result.items():\n",
    "            print(f\"{item}: {count}\")\n",
    "        metrics = evaluate_classification_metrics(y_valid, predicted_labels)\n",
    "        print(metrics)\n",
    "        \n",
    "        accuracy.append(metrics['Accuracy'])\n",
    "        print()\n",
    "    return np.mean(accuracy), np.std(accuracy)\n",
    "\n",
    "def cv_for_test_set_eval(dict_fault, X, freq, splits):\n",
    "    tscv = TimeSeriesSplit(n_splits=splits)  # You can adjust the number of splits\n",
    "    results = {}\n",
    "    counter = 1\n",
    "    for file in dict_fault.keys():\n",
    "        if 'short' in file:\n",
    "            continue\n",
    "        else:\n",
    "            results[file] = {}\n",
    "            results[file]['Accuracy'] = []\n",
    "            results[file]['F1'] = []\n",
    "            results[file]['Precision'] = []\n",
    "            results[file]['Recall'] = []\n",
    "            \n",
    "    for train_index, test_index in tscv.split(X):\n",
    "        \n",
    "        X_train = X[train_index]\n",
    "        train_fft = np.fft.fft(X_train)\n",
    "        scaler = MinMaxScaler()\n",
    "        train_scaled_fft = scaler.fit_transform(train_fft.real)\n",
    "        trained_lof_model = fine_tune_lof_model(train_scaled_fft)\n",
    "\n",
    "        for file in dict_fault.keys():\n",
    "            if 'short' in file:\n",
    "                continue\n",
    "            else:\n",
    "                df_test_resampled = dict_fault[file].resample(freq).mean()\n",
    "               \n",
    "                \n",
    "                X_test = df_test_resampled.values[test_index]\n",
    "                X_non_faulty = X[test_index]\n",
    "                \n",
    "                X_test_fault_non_fault = np.vstack((X_test, X_non_faulty))\n",
    "                \n",
    "                y_test = [1]*X_test.shape[0]\n",
    "                y_test.extend([0]*X_non_faulty.shape[0])\n",
    "                \n",
    "                test_fft = np.fft.fft(X_test_fault_non_fault)\n",
    "                test_pred_labels = trained_lof_model.predict(scaler.transform(test_fft.real))\n",
    "                predicted_labels = np.where(test_pred_labels == -1, 1, 0)\n",
    "                \n",
    "                metrics = evaluate_classification_metrics(y_test, predicted_labels)\n",
    "                print(counter, metrics)\n",
    "                results[file]['Accuracy'].append(metrics['Accuracy'])\n",
    "                results[file]['F1'].append(metrics['F1 Score'])\n",
    "                results[file]['Precision'].append(metrics['Precision'])\n",
    "                results[file]['Recall'].append(metrics['Recall'])\n",
    "        print('*'*100)\n",
    "        counter+=1\n",
    "    return results\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "\n",
    "def drop_low_variance_features(df, threshold=0.01):\n",
    "    \"\"\"\n",
    "    Drop low-variance features from a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): Input DataFrame.\n",
    "    - threshold (float): Variance threshold below which features will be dropped.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame with low-variance features dropped.\n",
    "    - list: Names of dropped features.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize VarianceThreshold with the specified threshold\n",
    "    selector = VarianceThreshold(threshold)\n",
    "\n",
    "    # Fit the selector on the features\n",
    "    selector.fit(df)\n",
    "\n",
    "    # Get boolean mask indicating selected features\n",
    "    selected_features = selector.get_support()\n",
    "\n",
    "    # Get the names of the dropped features\n",
    "    dropped_features = df.columns[~selected_features].tolist()\n",
    "\n",
    "    # Drop low-variance features from the original DataFrame\n",
    "    df_filtered = df.drop(columns=dropped_features)\n",
    "\n",
    "    # Print information about dropped features\n",
    "    if dropped_features:\n",
    "        print(f\"Dropped features with low variance: {', '.join(dropped_features)}\")\n",
    "    else:\n",
    "        print(\"No features were dropped.\")\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "def plot_features(df_correct, dict_fault, feature):\n",
    "\n",
    "    for file in dict_fault.keys():\n",
    "        print(file)\n",
    "        plt.plot(df_correct[feature].values, 'g')\n",
    "        plt.plot(dict_fault[file][feature].values, 'r')\n",
    "        plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "450b78e2-2ca5-47e6-aab9-2707f4608ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_data = load_data('AHU_annual.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d4699f1-8dc4-4766-b025-71cab2a077a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped features with low variance: OA_CFM, SA_SPSPT, SA_TEMPSPT, SF_SPD\n"
     ]
    }
   ],
   "source": [
    "#df_correct_dropped_variance_features = drop_low_variance_features(correct_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fc933d8-6d55-4141-8754-c1f6a5a41f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#selected_features = list(df_correct_dropped_variance_features.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4807aa7d-7be6-4c5a-802c-2cfd06e8ed61",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_faulty_df = {}\n",
    "for c, file in enumerate(os.listdir(base_dir)):\n",
    "    if c>0:\n",
    "        dict_faulty_df[file] = load_data(file)\n",
    "        #dict_faulty_df[file] = df[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1aa0cb7-bae7-48a9-8153-93b89b1d6f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_start_date_time = '2018-01-01 01:00:00'\n",
    "training_end_date_time = '2018-10-31 23:59:00'\n",
    "\n",
    "test_start_data = '2018-11-01 00:00:00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "37b5b0b3-c402-460b-a5bb-990e6aaf8f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "resampling_freq = '5T'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c874f4a5-7864-4156-9ecd-835065501f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_resampled = correct_data.resample(resampling_freq).mean()\n",
    "X = df_train_resampled.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11188c3-d823-44ba-ab90-b232d010a54a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "TP: 6616\n",
      "TN: 8883\n",
      "FP: 672\n",
      "FN: 2939\n",
      "1 {'Accuracy': 0.81, 'Precision': 1, 'Recall': 0.69, 'F1 Score': 0.79}\n",
      "TP: 5975\n",
      "TN: 8883\n",
      "FP: 672\n",
      "FN: 3580\n",
      "1 {'Accuracy': 0.78, 'Precision': 1, 'Recall': 0.63, 'F1 Score': 0.74}\n",
      "TP: 6962\n",
      "TN: 8883\n",
      "FP: 672\n",
      "FN: 2593\n",
      "1 {'Accuracy': 0.83, 'Precision': 1, 'Recall': 0.73, 'F1 Score': 0.81}\n",
      "TP: 6650\n",
      "TN: 8883\n",
      "FP: 672\n",
      "FN: 2905\n",
      "1 {'Accuracy': 0.81, 'Precision': 1, 'Recall': 0.7, 'F1 Score': 0.79}\n",
      "TP: 6943\n",
      "TN: 8883\n",
      "FP: 672\n",
      "FN: 2612\n",
      "1 {'Accuracy': 0.83, 'Precision': 1, 'Recall': 0.73, 'F1 Score': 0.81}\n",
      "TP: 6943\n",
      "TN: 8883\n",
      "FP: 672\n",
      "FN: 2612\n",
      "1 {'Accuracy': 0.83, 'Precision': 1, 'Recall': 0.73, 'F1 Score': 0.81}\n",
      "TP: 6943\n",
      "TN: 8883\n",
      "FP: 672\n",
      "FN: 2612\n",
      "1 {'Accuracy': 0.83, 'Precision': 1, 'Recall': 0.73, 'F1 Score': 0.81}\n",
      "TP: 6943\n",
      "TN: 8883\n",
      "FP: 672\n",
      "FN: 2612\n",
      "1 {'Accuracy': 0.83, 'Precision': 1, 'Recall': 0.73, 'F1 Score': 0.81}\n",
      "TP: 5900\n",
      "TN: 8883\n",
      "FP: 672\n",
      "FN: 3655\n",
      "1 {'Accuracy': 0.77, 'Precision': 1, 'Recall': 0.62, 'F1 Score': 0.73}\n",
      "TP: 5928\n",
      "TN: 8883\n",
      "FP: 672\n",
      "FN: 3627\n",
      "1 {'Accuracy': 0.78, 'Precision': 1, 'Recall': 0.62, 'F1 Score': 0.73}\n",
      "TP: 6608\n",
      "TN: 8883\n",
      "FP: 672\n",
      "FN: 2947\n",
      "1 {'Accuracy': 0.81, 'Precision': 1, 'Recall': 0.69, 'F1 Score': 0.79}\n",
      "TP: 6536\n",
      "TN: 8883\n",
      "FP: 672\n",
      "FN: 3019\n",
      "1 {'Accuracy': 0.81, 'Precision': 1, 'Recall': 0.68, 'F1 Score': 0.78}\n",
      "TP: 8405\n",
      "TN: 8883\n",
      "FP: 672\n",
      "FN: 1150\n",
      "1 {'Accuracy': 0.9, 'Precision': 1, 'Recall': 0.88, 'F1 Score': 0.9}\n",
      "TP: 8182\n",
      "TN: 8883\n",
      "FP: 672\n",
      "FN: 1373\n",
      "1 {'Accuracy': 0.89, 'Precision': 1, 'Recall': 0.86, 'F1 Score': 0.89}\n",
      "TP: 5419\n",
      "TN: 8883\n",
      "FP: 672\n",
      "FN: 4136\n",
      "1 {'Accuracy': 0.75, 'Precision': 1, 'Recall': 0.57, 'F1 Score': 0.69}\n",
      "TP: 6942\n",
      "TN: 8883\n",
      "FP: 672\n",
      "FN: 2613\n",
      "1 {'Accuracy': 0.83, 'Precision': 1, 'Recall': 0.73, 'F1 Score': 0.81}\n",
      "TP: 6942\n",
      "TN: 8883\n",
      "FP: 672\n",
      "FN: 2613\n",
      "1 {'Accuracy': 0.83, 'Precision': 1, 'Recall': 0.73, 'F1 Score': 0.81}\n",
      "TP: 6942\n",
      "TN: 8883\n",
      "FP: 672\n",
      "FN: 2613\n",
      "1 {'Accuracy': 0.83, 'Precision': 1, 'Recall': 0.73, 'F1 Score': 0.81}\n",
      "TP: 6942\n",
      "TN: 8883\n",
      "FP: 672\n",
      "FN: 2613\n",
      "1 {'Accuracy': 0.83, 'Precision': 1, 'Recall': 0.73, 'F1 Score': 0.81}\n",
      "****************************************************************************************************\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "TP: 5563\n",
      "TN: 8070\n",
      "FP: 1485\n",
      "FN: 3992\n",
      "2 {'Accuracy': 0.71, 'Precision': 1, 'Recall': 0.58, 'F1 Score': 0.67}\n",
      "TP: 5376\n",
      "TN: 8070\n",
      "FP: 1485\n",
      "FN: 4179\n",
      "2 {'Accuracy': 0.7, 'Precision': 1, 'Recall': 0.56, 'F1 Score': 0.65}\n",
      "TP: 5898\n",
      "TN: 8070\n",
      "FP: 1485\n",
      "FN: 3657\n",
      "2 {'Accuracy': 0.73, 'Precision': 1, 'Recall': 0.62, 'F1 Score': 0.7}\n",
      "TP: 6126\n",
      "TN: 8070\n",
      "FP: 1485\n",
      "FN: 3429\n",
      "2 {'Accuracy': 0.74, 'Precision': 1, 'Recall': 0.64, 'F1 Score': 0.71}\n",
      "TP: 5525\n",
      "TN: 8070\n",
      "FP: 1485\n",
      "FN: 4030\n",
      "2 {'Accuracy': 0.71, 'Precision': 1, 'Recall': 0.58, 'F1 Score': 0.67}\n",
      "TP: 5525\n",
      "TN: 8070\n",
      "FP: 1485\n",
      "FN: 4030\n",
      "2 {'Accuracy': 0.71, 'Precision': 1, 'Recall': 0.58, 'F1 Score': 0.67}\n",
      "TP: 5525\n",
      "TN: 8070\n",
      "FP: 1485\n",
      "FN: 4030\n",
      "2 {'Accuracy': 0.71, 'Precision': 1, 'Recall': 0.58, 'F1 Score': 0.67}\n",
      "TP: 5525\n",
      "TN: 8070\n",
      "FP: 1485\n",
      "FN: 4030\n",
      "2 {'Accuracy': 0.71, 'Precision': 1, 'Recall': 0.58, 'F1 Score': 0.67}\n",
      "TP: 5323\n",
      "TN: 8070\n",
      "FP: 1485\n",
      "FN: 4232\n",
      "2 {'Accuracy': 0.7, 'Precision': 1, 'Recall': 0.56, 'F1 Score': 0.65}\n",
      "TP: 5327\n",
      "TN: 8070\n",
      "FP: 1485\n",
      "FN: 4228\n",
      "2 {'Accuracy': 0.7, 'Precision': 1, 'Recall': 0.56, 'F1 Score': 0.65}\n",
      "TP: 5506\n",
      "TN: 8070\n",
      "FP: 1485\n",
      "FN: 4049\n",
      "2 {'Accuracy': 0.71, 'Precision': 1, 'Recall': 0.58, 'F1 Score': 0.67}\n",
      "TP: 5668\n",
      "TN: 8070\n",
      "FP: 1485\n",
      "FN: 3887\n",
      "2 {'Accuracy': 0.72, 'Precision': 1, 'Recall': 0.59, 'F1 Score': 0.68}\n",
      "TP: 8959\n",
      "TN: 8070\n",
      "FP: 1485\n",
      "FN: 596\n",
      "2 {'Accuracy': 0.89, 'Precision': 1, 'Recall': 0.94, 'F1 Score': 0.9}\n",
      "TP: 8861\n",
      "TN: 8070\n",
      "FP: 1485\n",
      "FN: 694\n",
      "2 {'Accuracy': 0.89, 'Precision': 1, 'Recall': 0.93, 'F1 Score': 0.89}\n",
      "TP: 5697\n",
      "TN: 8070\n",
      "FP: 1485\n",
      "FN: 3858\n",
      "2 {'Accuracy': 0.72, 'Precision': 1, 'Recall': 0.6, 'F1 Score': 0.68}\n",
      "TP: 5742\n",
      "TN: 8070\n",
      "FP: 1485\n",
      "FN: 3813\n",
      "2 {'Accuracy': 0.72, 'Precision': 1, 'Recall': 0.6, 'F1 Score': 0.68}\n",
      "TP: 5742\n",
      "TN: 8070\n",
      "FP: 1485\n",
      "FN: 3813\n",
      "2 {'Accuracy': 0.72, 'Precision': 1, 'Recall': 0.6, 'F1 Score': 0.68}\n",
      "TP: 5742\n",
      "TN: 8070\n",
      "FP: 1485\n",
      "FN: 3813\n",
      "2 {'Accuracy': 0.72, 'Precision': 1, 'Recall': 0.6, 'F1 Score': 0.68}\n",
      "TP: 5742\n",
      "TN: 8070\n",
      "FP: 1485\n",
      "FN: 3813\n",
      "2 {'Accuracy': 0.72, 'Precision': 1, 'Recall': 0.6, 'F1 Score': 0.68}\n",
      "****************************************************************************************************\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "TP: 7202\n",
      "TN: 6527\n",
      "FP: 3028\n",
      "FN: 2353\n",
      "3 {'Accuracy': 0.72, 'Precision': 1, 'Recall': 0.75, 'F1 Score': 0.73}\n",
      "TP: 7222\n",
      "TN: 6527\n",
      "FP: 3028\n",
      "FN: 2333\n",
      "3 {'Accuracy': 0.72, 'Precision': 1, 'Recall': 0.76, 'F1 Score': 0.73}\n",
      "TP: 7689\n",
      "TN: 6527\n",
      "FP: 3028\n",
      "FN: 1866\n",
      "3 {'Accuracy': 0.74, 'Precision': 1, 'Recall': 0.8, 'F1 Score': 0.76}\n"
     ]
    }
   ],
   "source": [
    "test_set_cv_results = cv_for_test_set_eval(dict_faulty_df, X, resampling_freq, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729e91fe-4dbc-4ac0-a989-baa4ad91c898",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/10_fold_faults_non_faults_evaluation_features_dropped.json', \"w\") as outfile: \n",
    "        json.dump(test_set_cv_results, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542cc06e-3c47-484b-b27e-d2687852dcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the model to a file\n",
    "joblib.dump(trained_lof_model, 'models/lof_model_'+str(resampling_freq)+'.pkl')\n",
    "joblib.dump(scaler, 'models/minmax_model_'+str(resampling_freq)+'.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87af85f8-d3ba-44b2-83f0-fe770c93c02c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abcfb5f-b74e-429d-a49a-a6f804c843c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
